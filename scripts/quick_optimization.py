import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, classification_report
import xgboost as xgb
import catboost as cb
import shap
import joblib
import time
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

print("=== ë¹ ë¥¸ ëª¨ë¸ ìµœì í™” ë° SHAP ë¶„ì„ ===")

class QuickOptimizationSystem:
    """ë¹ ë¥¸ ìµœì í™” ì‹œìŠ¤í…œ - ì œí•œëœ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì‚¬ìš©"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.best_models = {}
        self.optimization_results = {}
        
    def load_and_prepare_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        df = pd.read_csv('processed_data/dataset_with_unified_occupations.csv')
        
        # ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì¼€ì´ìŠ¤ë§Œ ì„ ë³„
        df_sorted = df.sort_values(['pid', 'year']).reset_index(drop=True)
        
        # ë‹¤ìŒ ì—°ë„ íƒ€ê²Ÿ ìƒì„±
        df_sorted['next_year'] = df_sorted.groupby('pid')['year'].shift(-1)
        df_sorted['next_wage'] = df_sorted.groupby('pid')['p_wage'].shift(-1)
        df_sorted['next_satisfaction'] = df_sorted.groupby('pid')['p4321'].shift(-1)
        
        # ì—°ì†ëœ ì—°ë„ë§Œ í•„í„°ë§
        consecutive_mask = (df_sorted['next_year'] == df_sorted['year'] + 1)
        df_consecutive = df_sorted[consecutive_mask].copy()
        
        # íƒ€ê²Ÿì´ ëª¨ë‘ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ ì„ ë³„
        valid_mask = (~df_consecutive['next_wage'].isna()) & (~df_consecutive['next_satisfaction'].isna())
        df_final = df_consecutive[valid_mask].copy()
        
        print(f"ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: {df_final.shape}")
        
        # íŠ¹ì„± ì„ íƒ
        exclude_cols = ['pid', 'year', 'next_year', 'next_wage', 'next_satisfaction', 'p_wage', 'p4321']
        feature_cols = [col for col in df_final.columns if col not in exclude_cols]
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        le_dict = {}
        for col in df_final[feature_cols].select_dtypes(include=['object']).columns:
            le = LabelEncoder()
            df_final[col] = le.fit_transform(df_final[col].astype(str))
            le_dict[col] = le
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df_final[feature_cols] = df_final[feature_cols].fillna(df_final[feature_cols].median())
        
        # ì‹œê°„ ê¸°ë°˜ ë¶„í• 
        train_mask = df_final['year'] <= 2020
        test_mask = df_final['year'] >= 2021
        
        self.X_train = df_final[train_mask][feature_cols]
        self.X_test = df_final[test_mask][feature_cols]
        self.y_wage_train = df_final[train_mask]['next_wage']
        self.y_wage_test = df_final[test_mask]['next_wage']
        self.y_sat_train = df_final[train_mask]['next_satisfaction'].astype(int)
        self.y_sat_test = df_final[test_mask]['next_satisfaction'].astype(int)
        
        self.feature_names = feature_cols
        
        print(f"í›ˆë ¨ ë°ì´í„°: {self.X_train.shape[0]}ê°œ")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {self.X_test.shape[0]}ê°œ")
        print(f"íŠ¹ì„± ê°œìˆ˜: {len(self.feature_names)}ê°œ")
    
    def optimize_catboost_quick(self):
        """CatBoost ë¹ ë¥¸ ìµœì í™”"""
        print("\nCatBoost ì„ê¸ˆ ì˜ˆì¸¡ ëª¨ë¸ ë¹ ë¥¸ ìµœì í™”...")
        
        # ì œí•œëœ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
        param_grid = {
            'iterations': [500, 1000],
            'learning_rate': [0.05, 0.1, 0.2],
            'depth': [6, 8],
            'l2_leaf_reg': [3, 5]
        }
        
        catboost_model = cb.CatBoostRegressor(
            random_seed=self.random_state,
            verbose=False
        )
        
        # ì‹œê³„ì—´ êµì°¨ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=3)
        
        grid_search = GridSearchCV(
            catboost_model, 
            param_grid, 
            cv=tscv,
            scoring='neg_mean_squared_error',
            n_jobs=-1
        )
        
        grid_search.fit(self.X_train, self.y_wage_train)
        
        self.best_models['catboost_wage'] = grid_search.best_estimator_
        
        # ì„±ëŠ¥ í‰ê°€
        y_pred_train = self.best_models['catboost_wage'].predict(self.X_train)
        y_pred_test = self.best_models['catboost_wage'].predict(self.X_test)
        
        self.optimization_results['catboost_wage'] = {
            'best_params': grid_search.best_params_,
            'best_score': -grid_search.best_score_,
            'train_rmse': np.sqrt(mean_squared_error(self.y_wage_train, y_pred_train)),
            'test_rmse': np.sqrt(mean_squared_error(self.y_wage_test, y_pred_test)),
            'train_r2': r2_score(self.y_wage_train, y_pred_train),
            'test_r2': r2_score(self.y_wage_test, y_pred_test),
            'train_mae': mean_absolute_error(self.y_wage_train, y_pred_train),
            'test_mae': mean_absolute_error(self.y_wage_test, y_pred_test)
        }
        
        result = self.optimization_results['catboost_wage']
        print(f"âœ“ CatBoost ìµœì í™” ì™„ë£Œ")
        print(f"  ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
        print(f"  í…ŒìŠ¤íŠ¸ RMSE: {result['test_rmse']:.2f}")
        print(f"  í…ŒìŠ¤íŠ¸ RÂ²: {result['test_r2']:.4f}")
    
    def optimize_xgboost_quick(self):
        """XGBoost ë¹ ë¥¸ ìµœì í™”"""
        print("\nXGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ ëª¨ë¸ ë¹ ë¥¸ ìµœì í™”...")
        
        # ì œí•œëœ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
        param_grid = {
            'n_estimators': [500, 1000],
            'max_depth': [4, 6, 8],
            'learning_rate': [0.05, 0.1, 0.2],
            'subsample': [0.8, 0.9]
        }
        
        xgb_model = xgb.XGBClassifier(
            random_state=self.random_state,
            objective='multi:softprob',
            eval_metric='mlogloss',
            verbosity=0
        )
        
        # ì‹œê³„ì—´ êµì°¨ê²€ì¦
        tscv = TimeSeriesSplit(n_splits=3)
        
        grid_search = GridSearchCV(
            xgb_model, 
            param_grid, 
            cv=tscv,
            scoring='accuracy',
            n_jobs=-1
        )
        
        grid_search.fit(self.X_train, self.y_sat_train)
        
        self.best_models['xgboost_satisfaction'] = grid_search.best_estimator_
        
        # ì„±ëŠ¥ í‰ê°€
        y_pred_train = self.best_models['xgboost_satisfaction'].predict(self.X_train)
        y_pred_test = self.best_models['xgboost_satisfaction'].predict(self.X_test)
        
        self.optimization_results['xgboost_satisfaction'] = {
            'best_params': grid_search.best_params_,
            'best_score': grid_search.best_score_,
            'train_accuracy': accuracy_score(self.y_sat_train, y_pred_train),
            'test_accuracy': accuracy_score(self.y_sat_test, y_pred_test),
            'train_report': classification_report(self.y_sat_train, y_pred_train, output_dict=True),
            'test_report': classification_report(self.y_sat_test, y_pred_test, output_dict=True)
        }
        
        result = self.optimization_results['xgboost_satisfaction']
        print(f"âœ“ XGBoost ìµœì í™” ì™„ë£Œ")
        print(f"  ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
        print(f"  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {result['test_accuracy']:.4f}")
    
    def perform_shap_analysis(self):
        """ë¹ ë¥¸ SHAP ë¶„ì„"""
        print("\nSHAP ë¶„ì„ ì‹œì‘...")
        
        # ìƒ˜í”Œ ì„ íƒ (ë¹ ë¥¸ ë¶„ì„)
        sample_size = min(300, len(self.X_test))
        sample_indices = np.random.choice(len(self.X_test), sample_size, replace=False)
        X_shap = self.X_test.iloc[sample_indices]
        
        # CatBoost SHAP
        if 'catboost_wage' in self.best_models:
            print("  CatBoost SHAP ë¶„ì„...")
            explainer = shap.Explainer(self.best_models['catboost_wage'])
            shap_values = explainer(X_shap.iloc[:100])  # ë” ì‘ì€ ìƒ˜í”Œ
            
            # Top 10 íŠ¹ì„± ì¤‘ìš”ë„
            feature_importance = np.abs(shap_values.values).mean(0)
            top_features_idx = np.argsort(feature_importance)[-10:][::-1]
            
            print("    ì„ê¸ˆ ì˜ˆì¸¡ ì£¼ìš” íŠ¹ì„± (ìƒìœ„ 10ê°œ):")
            for i, idx in enumerate(top_features_idx):
                feature_name = self.feature_names[idx]
                importance = feature_importance[idx]
                print(f"      {i+1}. {feature_name}: {importance:.4f}")
        
        # XGBoost SHAP
        if 'xgboost_satisfaction' in self.best_models:
            print("  XGBoost SHAP ë¶„ì„...")
            explainer = shap.Explainer(self.best_models['xgboost_satisfaction'])
            shap_values = explainer(X_shap.iloc[:100])
            
            # ë‹¤ì¤‘ í´ë˜ìŠ¤ì˜ ê²½ìš° ì²« ë²ˆì§¸ í´ë˜ìŠ¤ì˜ ì¤‘ìš”ë„ ì‚¬ìš©
            if len(shap_values.values.shape) == 3:
                feature_importance = np.abs(shap_values.values[:, :, 0]).mean(0)
            else:
                feature_importance = np.abs(shap_values.values).mean(0)
                
            top_features_idx = np.argsort(feature_importance)[-10:][::-1]
            
            print("    ë§Œì¡±ë„ ì˜ˆì¸¡ ì£¼ìš” íŠ¹ì„± (ìƒìœ„ 10ê°œ):")
            for i, idx in enumerate(top_features_idx):
                feature_name = self.feature_names[idx]
                importance = feature_importance[idx]
                print(f"      {i+1}. {feature_name}: {importance:.4f}")
    
    def save_models_and_results(self):
        """ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥"""
        print("\nëª¨ë¸ ë° ê²°ê³¼ ì €ì¥...")
        
        import os
        os.makedirs('models', exist_ok=True)
        os.makedirs('model_results', exist_ok=True)
        
        for model_name, model in self.best_models.items():
            joblib.dump(model, f'models/quick_{model_name}.pkl')
            print(f"  âœ“ {model_name} ì €ì¥")
        
        results_df = pd.DataFrame(self.optimization_results).T
        results_df.to_csv('model_results/quick_optimization_results.csv')
        print(f"  âœ“ ê²°ê³¼ ì €ì¥")
    
    def generate_performance_report(self):
        """ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*60)
        print("ë¹ ë¥¸ ëª¨ë¸ ìµœì í™” ê²°ê³¼ ë³´ê³ ì„œ")
        print("="*60)
        
        if 'catboost_wage' in self.optimization_results:
            result = self.optimization_results['catboost_wage']
            print(f"\nğŸ’° CatBoost ì„ê¸ˆ ì˜ˆì¸¡:")
            print(f"   í…ŒìŠ¤íŠ¸ RMSE: {result['test_rmse']:.2f}ë§Œì›")
            print(f"   í…ŒìŠ¤íŠ¸ RÂ²: {result['test_r2']:.4f}")
            print(f"   í…ŒìŠ¤íŠ¸ MAE: {result['test_mae']:.2f}ë§Œì›")
            print(f"   ê¸°ì¡´ ëŒ€ë¹„: RMSE {115.92:.2f} â†’ {result['test_rmse']:.2f}")
        
        if 'xgboost_satisfaction' in self.optimization_results:
            result = self.optimization_results['xgboost_satisfaction']
            print(f"\nğŸ˜Š XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡:")
            print(f"   í…ŒìŠ¤íŠ¸ ì •í™•ë„: {result['test_accuracy']:.4f}")
            print(f"   ê¸°ì¡´ ëŒ€ë¹„: 0.694 â†’ {result['test_accuracy']:.4f}")
        
        print(f"\nğŸ“Š ë¶„ì„ ì™„ë£Œ ì‹œê° ì •ë³´:")
        print(f"   - ëª¨ë¸: models/ í´ë”")
        print(f"   - ê²°ê³¼: model_results/quick_optimization_results.csv")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    start_time = time.time()
    
    optimizer = QuickOptimizationSystem(random_state=42)
    
    # ë°ì´í„° ë¡œë“œ
    optimizer.load_and_prepare_data()
    
    # ë¹ ë¥¸ ìµœì í™”
    optimizer.optimize_catboost_quick()
    optimizer.optimize_xgboost_quick()
    
    # SHAP ë¶„ì„
    optimizer.perform_shap_analysis()
    
    # ê²°ê³¼ ì €ì¥
    optimizer.save_models_and_results()
    
    # ë³´ê³ ì„œ
    optimizer.generate_performance_report()
    
    total_time = (time.time() - start_time) / 60
    print(f"\nğŸ‰ ì „ì²´ ìµœì í™” ì™„ë£Œ! ì†Œìš”ì‹œê°„: {total_time:.1f}ë¶„")

if __name__ == "__main__":
    main()