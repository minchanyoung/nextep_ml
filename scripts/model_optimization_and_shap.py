import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, TimeSeriesSplit
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, classification_report
import xgboost as xgb
import catboost as cb
import shap
import optuna
from optuna.samplers import TPESampler
import warnings
import joblib
import time
warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

print("=== ëª¨ë¸ ìµœì í™” ë° SHAP ë¶„ì„ ì‹œìŠ¤í…œ ===")

class ModelOptimizationSystem:
    """ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ë° í•´ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self, random_state=42):
        self.random_state = random_state
        self.best_models = {}
        self.optimization_results = {}
        self.shap_explainers = {}
        self.shap_values = {}
        
    def load_and_prepare_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ë°ì´í„° ë¡œë“œ ì¤‘...")
        
        # ìµœì¢… ML ë°ì´í„°ì…‹ ë¡œë“œ
        df = pd.read_csv('processed_data/dataset_with_unified_occupations.csv')
        
        # ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì¼€ì´ìŠ¤ë§Œ ì„ ë³„
        df_sorted = df.sort_values(['pid', 'year']).reset_index(drop=True)
        
        # ë‹¤ìŒ ì—°ë„ íƒ€ê²Ÿ ìƒì„±
        df_sorted['next_year'] = df_sorted.groupby('pid')['year'].shift(-1)
        df_sorted['next_wage'] = df_sorted.groupby('pid')['p_wage'].shift(-1)
        df_sorted['next_satisfaction'] = df_sorted.groupby('pid')['p4321'].shift(-1)
        
        # ì—°ì†ëœ ì—°ë„ë§Œ í•„í„°ë§
        consecutive_mask = (df_sorted['next_year'] == df_sorted['year'] + 1)
        df_consecutive = df_sorted[consecutive_mask].copy()
        
        # íƒ€ê²Ÿì´ ëª¨ë‘ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ ì„ ë³„
        valid_mask = (~df_consecutive['next_wage'].isna()) & (~df_consecutive['next_satisfaction'].isna())
        df_final = df_consecutive[valid_mask].copy()
        
        # ë§Œì¡±ë„ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ìœ íš¨í•œ ê°’(0 ì´ˆê³¼)ë§Œ í•„í„°ë§
        df_final = df_final[df_final['next_satisfaction'] > 0].copy()
        
        print(f"ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: {df_final.shape}")
        
        # íŠ¹ì„± ì„ íƒ (íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸)
        exclude_cols = ['pid', 'year', 'next_year', 'next_wage', 'next_satisfaction', 
                       'p_wage', 'p4321']
        feature_cols = [col for col in df_final.columns if col not in exclude_cols]
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        le_dict = {}
        for col in df_final[feature_cols].select_dtypes(include=['object']).columns:
            le = LabelEncoder()
            df_final[col] = le.fit_transform(df_final[col].astype(str))
            le_dict[col] = le
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df_final[feature_cols] = df_final[feature_cols].fillna(df_final[feature_cols].median())
        
        # ì‹œê°„ ê¸°ë°˜ ë¶„í•  (2000-2020: í›ˆë ¨, 2021-2022: í…ŒìŠ¤íŠ¸)
        train_mask = df_final['year'] <= 2020
        test_mask = df_final['year'] >= 2021
        
        self.X_train = df_final[train_mask][feature_cols]
        self.X_test = df_final[test_mask][feature_cols]
        self.y_wage_train = df_final[train_mask]['next_wage']
        self.y_wage_test = df_final[test_mask]['next_wage']
        self.y_sat_train = (df_final[train_mask]['next_satisfaction'] - 1).astype(int)
        self.y_sat_test = (df_final[test_mask]['next_satisfaction'] - 1).astype(int)
        
        self.feature_names = feature_cols
        
        print(f"í›ˆë ¨ ë°ì´í„°: {self.X_train.shape[0]}ê°œ")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {self.X_test.shape[0]}ê°œ")
        print(f"íŠ¹ì„± ê°œìˆ˜: {len(self.feature_names)}ê°œ")
        
        return df_final
    
    def optimize_catboost_wage(self, n_trials=100):
        """CatBoost ì„ê¸ˆ ì˜ˆì¸¡ ëª¨ë¸ ìµœì í™”"""
        print(f"\nCatBoost ì„ê¸ˆ ì˜ˆì¸¡ ëª¨ë¸ ìµœì í™” ì‹œì‘ (ì‹œí–‰ íšŸìˆ˜: {n_trials})")
        
        def objective(trial):
            params = {
                'iterations': trial.suggest_int('iterations', 500, 2000),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'depth': trial.suggest_int('depth', 4, 10),
                'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),
                'border_count': trial.suggest_int('border_count', 32, 255),
                'random_seed': self.random_state,
                'verbose': False
            }
            
            # ì‹œê³„ì—´ êµì°¨ê²€ì¦
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []
            
            for train_idx, val_idx in tscv.split(self.X_train):
                X_fold_train = self.X_train.iloc[train_idx]
                X_fold_val = self.X_train.iloc[val_idx]
                y_fold_train = self.y_wage_train.iloc[train_idx]
                y_fold_val = self.y_wage_train.iloc[val_idx]
                
                model = cb.CatBoostRegressor(**params)
                model.fit(X_fold_train, y_fold_train)
                
                y_pred = model.predict(X_fold_val)
                rmse = np.sqrt(mean_squared_error(y_fold_val, y_pred))
                scores.append(rmse)
            
            return np.mean(scores)
        
        study = optuna.create_study(direction='minimize', sampler=TPESampler())
        study.optimize(objective, n_trials=n_trials)
        
        # ìµœì  ëª¨ë¸ í›ˆë ¨
        best_params = study.best_params
        best_params['random_seed'] = self.random_state
        best_params['verbose'] = False
        
        self.best_models['catboost_wage'] = cb.CatBoostRegressor(**best_params)
        self.best_models['catboost_wage'].fit(self.X_train, self.y_wage_train)
        
        # ì„±ëŠ¥ í‰ê°€
        y_pred_train = self.best_models['catboost_wage'].predict(self.X_train)
        y_pred_test = self.best_models['catboost_wage'].predict(self.X_test)
        
        self.optimization_results['catboost_wage'] = {
            'best_params': best_params,
            'best_score': study.best_value,
            'train_rmse': np.sqrt(mean_squared_error(self.y_wage_train, y_pred_train)),
            'test_rmse': np.sqrt(mean_squared_error(self.y_wage_test, y_pred_test)),
            'train_r2': r2_score(self.y_wage_train, y_pred_train),
            'test_r2': r2_score(self.y_wage_test, y_pred_test),
            'train_mae': mean_absolute_error(self.y_wage_train, y_pred_train),
            'test_mae': mean_absolute_error(self.y_wage_test, y_pred_test)
        }
        
        print(f"CatBoost ì„ê¸ˆ ì˜ˆì¸¡ ìµœì í™” ì™„ë£Œ")
        print(f"ìµœì  ê²€ì¦ RMSE: {study.best_value:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ RMSE: {self.optimization_results['catboost_wage']['test_rmse']:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ RÂ²: {self.optimization_results['catboost_wage']['test_r2']:.4f}")
        
        return study
    
    def optimize_xgboost_satisfaction(self, n_trials=100):
        """XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ ëª¨ë¸ ìµœì í™”"""
        print(f"\nXGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ ëª¨ë¸ ìµœì í™” ì‹œì‘ (ì‹œí–‰ íšŸìˆ˜: {n_trials})")
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 500, 2000),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),
                'random_state': self.random_state,
                'objective': 'multi:softprob',
                'eval_metric': 'mlogloss',
                'verbosity': 0
            }
            
            # ì‹œê³„ì—´ êµì°¨ê²€ì¦
            tscv = TimeSeriesSplit(n_splits=3)
            scores = []
            
            for train_idx, val_idx in tscv.split(self.X_train):
                X_fold_train = self.X_train.iloc[train_idx]
                X_fold_val = self.X_train.iloc[val_idx]
                y_fold_train = self.y_sat_train.iloc[train_idx]
                y_fold_val = self.y_sat_train.iloc[val_idx]
                
                model = xgb.XGBClassifier(**params)
                model.fit(X_fold_train, y_fold_train)
                
                y_pred = model.predict(X_fold_val)
                accuracy = accuracy_score(y_fold_val, y_pred)
                scores.append(accuracy)
            
            return -np.mean(scores)  # OptunaëŠ” ìµœì†Œí™” ë°©í–¥ì´ë¯€ë¡œ ìŒìˆ˜ ì‚¬ìš©
        
        study = optuna.create_study(direction='minimize', sampler=TPESampler())
        study.optimize(objective, n_trials=n_trials)
        
        # ìµœì  ëª¨ë¸ í›ˆë ¨
        best_params = study.best_params
        best_params['random_state'] = self.random_state
        best_params['objective'] = 'multi:softprob'
        best_params['eval_metric'] = 'mlogloss'
        best_params['verbosity'] = 0
        
        self.best_models['xgboost_satisfaction'] = xgb.XGBClassifier(**best_params)
        self.best_models['xgboost_satisfaction'].fit(self.X_train, self.y_sat_train)
        
        # ì„±ëŠ¥ í‰ê°€
        y_pred_train = self.best_models['xgboost_satisfaction'].predict(self.X_train)
        y_pred_test = self.best_models['xgboost_satisfaction'].predict(self.X_test)
        
        self.optimization_results['xgboost_satisfaction'] = {
            'best_params': best_params,
            'best_score': -study.best_value,  # ë‹¤ì‹œ ì–‘ìˆ˜ë¡œ ë³€í™˜
            'train_accuracy': accuracy_score(self.y_sat_train, y_pred_train),
            'test_accuracy': accuracy_score(self.y_sat_test, y_pred_test),
            'train_report': classification_report(self.y_sat_train, y_pred_train, output_dict=True),
            'test_report': classification_report(self.y_sat_test, y_pred_test, output_dict=True)
        }
        
        print(f"XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ ìµœì í™” ì™„ë£Œ")
        print(f"ìµœì  ê²€ì¦ ì •í™•ë„: {-study.best_value:.4f}")
        print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {self.optimization_results['xgboost_satisfaction']['test_accuracy']:.4f}")
        
        return study
    
    def perform_shap_analysis(self, n_samples=1000):
        """SHAP ë¶„ì„ ìˆ˜í–‰"""
        print(f"\nSHAP ë¶„ì„ ì‹œì‘ (ìƒ˜í”Œ ìˆ˜: {n_samples})")
        
        # ìƒ˜í”Œ ì„ íƒ (ë¶„ì„ ì†ë„ í–¥ìƒ)
        sample_indices = np.random.choice(len(self.X_test), 
                                        min(n_samples, len(self.X_test)), 
                                        replace=False)
        X_shap = self.X_test.iloc[sample_indices]
        
        # CatBoost ì„ê¸ˆ ì˜ˆì¸¡ SHAP ë¶„ì„
        if 'catboost_wage' in self.best_models:
            print("CatBoost ì„ê¸ˆ ì˜ˆì¸¡ ëª¨ë¸ SHAP ë¶„ì„...")
            explainer_wage = shap.Explainer(self.best_models['catboost_wage'])
            shap_values_wage = explainer_wage(X_shap)
            
            self.shap_explainers['catboost_wage'] = explainer_wage
            self.shap_values['catboost_wage'] = shap_values_wage
        
        # XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ SHAP ë¶„ì„
        if 'xgboost_satisfaction' in self.best_models:
            print("XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ ëª¨ë¸ SHAP ë¶„ì„...")
            explainer_sat = shap.Explainer(self.best_models['xgboost_satisfaction'])
            shap_values_sat = explainer_sat(X_shap)
            
            self.shap_explainers['xgboost_satisfaction'] = explainer_sat
            self.shap_values['xgboost_satisfaction'] = shap_values_sat
        
        print("SHAP ë¶„ì„ ì™„ë£Œ")
    
    def create_shap_visualizations(self):
        """SHAP ì‹œê°í™” ìƒì„±"""
        print("SHAP ì‹œê°í™” ìƒì„± ì¤‘...")
        
        plt.style.use('default')
        
        # ì„ê¸ˆ ì˜ˆì¸¡ SHAP ì‹œê°í™”
        if 'catboost_wage' in self.shap_values:
            fig, axes = plt.subplots(2, 2, figsize=(20, 16))
            
            # Summary plot
            plt.sca(axes[0, 0])
            shap.summary_plot(self.shap_values['catboost_wage'], 
                            max_display=15, show=False)
            axes[0, 0].set_title('CatBoost ì„ê¸ˆ ì˜ˆì¸¡ - SHAP Summary Plot', fontsize=14, fontweight='bold')
            
            # Bar plot
            plt.sca(axes[0, 1])
            shap.summary_plot(self.shap_values['catboost_wage'], 
                            plot_type="bar", max_display=15, show=False)
            axes[0, 1].set_title('CatBoost ì„ê¸ˆ ì˜ˆì¸¡ - Feature Importance', fontsize=14, fontweight='bold')
            
            # Waterfall plot (ì²« ë²ˆì§¸ ì˜ˆì¸¡ ì‚¬ë¡€)
            plt.sca(axes[1, 0])
            shap.waterfall_plot(self.shap_values['catboost_wage'][0], show=False)
            axes[1, 0].set_title('CatBoost ì„ê¸ˆ ì˜ˆì¸¡ - Waterfall Plot (ì²« ë²ˆì§¸ ì‚¬ë¡€)', fontsize=14, fontweight='bold')
            
            # Force plotì„ ìœ„í•œ ê³µê°„
            axes[1, 1].text(0.5, 0.5, 'Interactive plotsëŠ”\nJupyter notebookì—ì„œ\ní™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤', 
                           ha='center', va='center', fontsize=12,
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue"))
            axes[1, 1].set_title('Force Plot ì•ˆë‚´', fontsize=14, fontweight='bold')
            axes[1, 1].set_xlim(0, 1)
            axes[1, 1].set_ylim(0, 1)
            axes[1, 1].axis('off')
            
            plt.tight_layout()
            plt.savefig('visualizations/shap_analysis_wage_catboost.png', dpi=300, bbox_inches='tight')
            plt.show()
        
        # ë§Œì¡±ë„ ì˜ˆì¸¡ SHAP ì‹œê°í™”
        if 'xgboost_satisfaction' in self.shap_values:
            fig, axes = plt.subplots(2, 2, figsize=(20, 16))
            
            # Summary plot
            plt.sca(axes[0, 0])
            shap.summary_plot(self.shap_values['xgboost_satisfaction'], 
                            max_display=15, show=False)
            axes[0, 0].set_title('XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ - SHAP Summary Plot', fontsize=14, fontweight='bold')
            
            # Bar plot
            plt.sca(axes[0, 1])
            shap.summary_plot(self.shap_values['xgboost_satisfaction'], 
                            plot_type="bar", max_display=15, show=False)
            axes[0, 1].set_title('XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ - Feature Importance', fontsize=14, fontweight='bold')
            
            # Waterfall plot
            plt.sca(axes[1, 0])
            shap.waterfall_plot(self.shap_values['xgboost_satisfaction'][0], show=False)
            axes[1, 0].set_title('XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ - Waterfall Plot (ì²« ë²ˆì§¸ ì‚¬ë¡€)', fontsize=14, fontweight='bold')
            
            # ì•ˆë‚´ ë©”ì‹œì§€
            axes[1, 1].text(0.5, 0.5, 'Multi-class SHAP valuesëŠ”\nê° í´ë˜ìŠ¤ë³„ë¡œ í•´ì„ë©ë‹ˆë‹¤\n\nìƒì„¸í•œ ë¶„ì„ì€ Jupyter\nnotebookì—ì„œ í™•ì¸ ê°€ëŠ¥', 
                           ha='center', va='center', fontsize=12,
                           bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgreen"))
            axes[1, 1].set_title('Multi-class SHAP ì•ˆë‚´', fontsize=14, fontweight='bold')
            axes[1, 1].set_xlim(0, 1)
            axes[1, 1].set_ylim(0, 1)
            axes[1, 1].axis('off')
            
            plt.tight_layout()
            plt.savefig('visualizations/shap_analysis_satisfaction_xgboost.png', dpi=300, bbox_inches='tight')
            plt.show()
    
    def save_models_and_results(self):
        """ëª¨ë¸ê³¼ ê²°ê³¼ ì €ì¥"""
        print("\nëª¨ë¸ ë° ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # ëª¨ë¸ ì €ì¥
        for model_name, model in self.best_models.items():
            joblib.dump(model, f'models/best_{model_name}.pkl')
            print(f"{model_name} ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
        
        # ê²°ê³¼ ì €ì¥
        results_df = pd.DataFrame(self.optimization_results).T
        results_df.to_csv('model_results/optimization_results.csv')
        print("ìµœì í™” ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
    
    def generate_performance_report(self):
        """ì¢…í•© ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±"""
        print("\n=== ëª¨ë¸ ìµœì í™” ë° ì„±ëŠ¥ ë¶„ì„ ë³´ê³ ì„œ ===\n")
        
        # CatBoost ì„ê¸ˆ ì˜ˆì¸¡ ê²°ê³¼
        if 'catboost_wage' in self.optimization_results:
            result = self.optimization_results['catboost_wage']
            print("ğŸ¯ CatBoost ì„ê¸ˆ ì˜ˆì¸¡ ëª¨ë¸")
            print("=" * 40)
            print(f"í…ŒìŠ¤íŠ¸ RMSE: {result['test_rmse']:.2f}ë§Œì›")
            print(f"í…ŒìŠ¤íŠ¸ MAE: {result['test_mae']:.2f}ë§Œì›")
            print(f"í…ŒìŠ¤íŠ¸ RÂ²: {result['test_r2']:.4f}")
            print(f"ìµœì  íŒŒë¼ë¯¸í„°: {result['best_params']}")
            print()
        
        # XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ ê²°ê³¼
        if 'xgboost_satisfaction' in self.optimization_results:
            result = self.optimization_results['xgboost_satisfaction']
            print("ğŸ˜Š XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ ëª¨ë¸")
            print("=" * 40)
            print(f"í…ŒìŠ¤íŠ¸ ì •í™•ë„: {result['test_accuracy']:.4f}")
            print("í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
            for class_name, metrics in result['test_report'].items():
                if isinstance(metrics, dict) and class_name not in ['accuracy', 'macro avg', 'weighted avg']:
                    print(f"  í´ë˜ìŠ¤ {class_name}: F1-score {metrics['f1-score']:.3f}")
            print(f"ìµœì  íŒŒë¼ë¯¸í„°: {result['best_params']}")
            print()
        
        # SHAP ë¶„ì„ ê²°ê³¼ ìš”ì•½
        print("ğŸ” SHAP ë¶„ì„ ê²°ê³¼")
        print("=" * 40)
        if 'catboost_wage' in self.shap_values:
            # ì„ê¸ˆ ì˜ˆì¸¡ ì¤‘ìš” íŠ¹ì„± top 5
            feature_importance = np.abs(self.shap_values['catboost_wage'].values).mean(0)
            top_features_idx = np.argsort(feature_importance)[-5:][::-1]
            
            print("ì„ê¸ˆ ì˜ˆì¸¡ ì£¼ìš” íŠ¹ì„± (ìƒìœ„ 5ê°œ):")
            for i, idx in enumerate(top_features_idx):
                feature_name = self.feature_names[idx]
                importance = feature_importance[idx]
                print(f"  {i+1}. {feature_name}: {importance:.4f}")
            print()
        
        if 'xgboost_satisfaction' in self.shap_values:
            # ë§Œì¡±ë„ ì˜ˆì¸¡ ì¤‘ìš” íŠ¹ì„± top 5 (ì²« ë²ˆì§¸ í´ë˜ìŠ¤ ê¸°ì¤€)
            if len(self.shap_values['xgboost_satisfaction'].values.shape) == 3:
                feature_importance = np.abs(self.shap_values['xgboost_satisfaction'].values[:, :, 0]).mean(0)
            else:
                feature_importance = np.abs(self.shap_values['xgboost_satisfaction'].values).mean(0)
                
            top_features_idx = np.argsort(feature_importance)[-5:][::-1]
            
            print("ë§Œì¡±ë„ ì˜ˆì¸¡ ì£¼ìš” íŠ¹ì„± (ìƒìœ„ 5ê°œ):")
            for i, idx in enumerate(top_features_idx):
                feature_name = self.feature_names[idx]
                importance = feature_importance[idx]
                print(f"  {i+1}. {feature_name}: {importance:.4f}")
            print()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²°ê³¼ ì €ì¥ í´ë” ìƒì„±
    import os
    os.makedirs('models', exist_ok=True)
    os.makedirs('model_results', exist_ok=True)
    os.makedirs('visualizations', exist_ok=True)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    optimizer = ModelOptimizationSystem(random_state=42)
    
    # ë°ì´í„° ë¡œë“œ
    data = optimizer.load_and_prepare_data()
    
    # ëª¨ë¸ ìµœì í™” (ì‹œí–‰ íšŸìˆ˜ë¥¼ ì ê²Œ ì„¤ì •í•˜ì—¬ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    print("í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘...")
    start_time = time.time()
    
    # CatBoost ì„ê¸ˆ ì˜ˆì¸¡ ìµœì í™”
    optimizer.optimize_catboost_wage(n_trials=50)  # ì‹¤ì œ ìš´ì˜ì‹œì—ëŠ” 100-200ìœ¼ë¡œ ì¦ê°€
    
    # XGBoost ë§Œì¡±ë„ ì˜ˆì¸¡ ìµœì í™”
    optimizer.optimize_xgboost_satisfaction(n_trials=50)  # ì‹¤ì œ ìš´ì˜ì‹œì—ëŠ” 100-200ìœ¼ë¡œ ì¦ê°€
    
    optimization_time = time.time() - start_time
    print(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì™„ë£Œ (ì†Œìš”ì‹œê°„: {optimization_time/60:.1f}ë¶„)")
    
    # SHAP ë¶„ì„
    optimizer.perform_shap_analysis(n_samples=500)  # ë¹ ë¥¸ ë¶„ì„ì„ ìœ„í•´ ìƒ˜í”Œ ìˆ˜ ì¡°ì •
    
    # ì‹œê°í™” ìƒì„±
    optimizer.create_shap_visualizations()
    
    # ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥
    optimizer.save_models_and_results()
    
    # ì„±ëŠ¥ ë³´ê³ ì„œ ìƒì„±
    optimizer.generate_performance_report()
    
    print(f"\nì „ì²´ ë¶„ì„ ì™„ë£Œ! ì´ ì†Œìš”ì‹œê°„: {(time.time() - start_time)/60:.1f}ë¶„")
    print("ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜:")
    print("- ëª¨ë¸: models/ í´ë”")
    print("- ì‹œê°í™”: visualizations/ í´ë”")
    print("- ì„±ëŠ¥ ê²°ê³¼: model_results/ í´ë”")

if __name__ == "__main__":
    main()